{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulbonin/RLclass_MVA/blob/main/Final_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This homework is due on April 3rd. Please upload a PDF file describing your investigations and the answers to the different questions (1. to 7.) here : https://nextcloud.univ-lille.fr/index.php/s/gdTY9KqEB2BADwL\n",
        "\n",
        "Please name your file NAME_FirstName.pdf"
      ],
      "metadata": {
        "id": "HT_4uQxbCv1L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn5vmAfVHAto"
      },
      "source": [
        "# Exploration-Exploitation in Reinforcement Learning\n",
        "\n",
        "In this homework, you will implement the **UCBVI** algorithm, for exploration in MDPs with finite states and actions and a **finite horizon** criterion. In a finite horizon criterion, the value function of a policy (starting from the $h$ step of the episode) is\n",
        "\n",
        "$$V_h^{\\pi}(s) = \\mathbb{E}^{\\pi}\\left[\\left.\\sum_{\\ell = h}^{H} \\gamma^{\\ell-h} r_{\\ell} \\right| s_h = s\\right]$$\n",
        "\n",
        "where the discount parameter $\\gamma \\in (0,1]$ is often set to $\\gamma = 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HjiuoEaDidG"
      },
      "source": [
        "### Useful libraries\n",
        "\n",
        "We will need a some tabular RL environments that are implement in the rlberry-scool library, that is developped within the Inria team Scool. You can  install it below (in a conda env if you do it locally)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rlberry-scool"
      ],
      "metadata": {
        "id": "iIxcMySYDsOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1mYv6U5nbI2"
      },
      "outputs": [],
      "source": [
        "# other useful imports\n",
        "import numpy as np\n",
        "import numba\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython.display import Video, Image\n",
        "from rlberry_scool.envs.finite import get_discrete_mountain_car_env\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsiPgClSnPzt"
      },
      "source": [
        "## Environment\n",
        "\n",
        "Our goal is to learn a good policy in a [Mountain Car](https://gymnasium.farama.org/environments/classic_control/mountain_car/) environment. The Mountain Car environement as implemented in gymnasium has a continuous state space. In order to apply UCBVI, we will use a discretized version from the rlberry library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df115JX-nNDZ"
      },
      "outputs": [],
      "source": [
        "def record_video(env, name, horizon=180,policy=None):\n",
        "    \"\"\"\n",
        "    input\n",
        "    horizon : length of the simulation\n",
        "    policy : either a determinstic policy represented by an (H,S) array\n",
        "    or a random policy which is uniform (None)\n",
        "    \"\"\"\n",
        "    env = deepcopy(env)\n",
        "    env = RecordVideo(env, \"./gym_videos/\"+name)\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "    tot_reward = 0\n",
        "    h = 0\n",
        "    while not done:\n",
        "        if policy is not None:\n",
        "            action = policy[h, s]\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        s, r, term, trunc, infos = env.step(action)\n",
        "        h += 1\n",
        "        tot_reward += r\n",
        "        done = (term or trunc) or h >= horizon\n",
        "    env.close()\n",
        "    print(\"Reward sum: {}\".format(tot_reward))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaleRewardWrapper(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, term, trunc, info = self.env.step(action)\n",
        "        if term:\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            reward = 0.0\n",
        "\n",
        "        return (next_state,reward,term,trunc,info)"
      ],
      "metadata": {
        "id": "prsCDCScP1oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the reward wrapper above, the agent gets a reward 0 in states that are not the top of the right hill. Once the goal state is reached, the agent stays in this state and gets a reward 1 until the end of the episode."
      ],
      "metadata": {
        "id": "Ilw4L0HrE_7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsHuqRTiO1p4"
      },
      "outputs": [],
      "source": [
        "env = ScaleRewardWrapper(get_discrete_mountain_car_env())\n",
        "record_video(env, name=\"Random-MountainCar\") #saves the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KULYE-mYDidI"
      },
      "outputs": [],
      "source": [
        "X = env.discretized_states[0,:] # discretized positions\n",
        "Xdot = env.discretized_states[1,:] # discretized velocities\n",
        "\n",
        "test = 67\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(\"state \",test,\" is \", (X[test],Xdot[test]))\n",
        "Video(\"gym_videos/Random-MountainCar/rl-video-episode-0.mp4\", embed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhRxLpONyenM"
      },
      "source": [
        "# Implementation of backward induction (i.e. value iteration)\n",
        "\n",
        "In a finite-horizon MDP, the optimal Bellman equations given a recursion that can be used to compute the optimal value function. We have $V_{H+1}^\\star(s) = 0$ for all $s$ and for $h \\leq H$,\n",
        "\n",
        "$$Q^\\star_h(s,a) = r(s,a) + \\gamma \\sum_{s\\prime \\in \\mathcal{S}} p(s\\prime|s,a) V^\\star_{h+1}(s\\prime) \\ \\ \\text{and } \\ \\ V^\\star_{h}(s) = \\max_{a \\in \\mathcal{A}} Q_h^\\star(s,a).$$\n",
        "\n",
        "Recall that the optimal policy is deterministic but *non-stationary* and satisfies $\\pi^\\star_h(s) = \\text{argmax}_{a} Q^\\star_h(s,a)$.\n",
        "\n",
        "**Complete the code below in order to compute the optimal Q function in a finite-horizon MDP.**\n",
        "\n",
        "Note that this code will also be useful to compute the policy used in each episode by UCB-VI, where we have to compute the optimal policy in an optimistic MDP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmDNWZ_Syc3w"
      },
      "outputs": [],
      "source": [
        "@numba.jit(nopython=True)  # use this to make the code much faster!\n",
        "\n",
        "def backward_induction(P, R, H, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        P: transition function (S,A,S)-dim matrix\n",
        "        R: reward function (S,A)-dim matrix\n",
        "        H: horizon\n",
        "        gamma: discount factor. Usually set to 1.0 in finite-horizon problems\n",
        "\n",
        "    Returns:\n",
        "        The optimal Q-function: array of shape (horizon, S,A)\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    V = np.zeros((H + 1, S))\n",
        "    Q = np.zeros((H+1,S,A))\n",
        "    for h in range(H-1, -1, -1):\n",
        "        for s in range(S):\n",
        "            ## TO BE COMPLETED WITH THE Q-VALUE COMPUTATION\n",
        "            # compute the value\n",
        "            V[h,s] = np.max(Q[h,s,:])\n",
        "            # ... and clip it (needed later in UCB-VI)\n",
        "            if (V[h, s] > H - h):\n",
        "                V[h, s] = H - h\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA8NpSmrDidI"
      },
      "source": [
        "You cannot try this function on the moutain car environement, as the expected rewards and transition probabilities cannot be easily computed, and are not embedded in the environment.\n",
        "\n",
        "However, you may check it on a simpler Gridworld environment, as P and R are properties of the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlndQUyF98As"
      },
      "outputs": [],
      "source": [
        "# Testing the implementation in a GridWorld\n",
        "from rlberry_scool.envs.finite import GridWorld\n",
        "\n",
        "test_env = GridWorld(nrows=8, ncols=8)\n",
        "test_env.renderer_type = \"pygame\"\n",
        "H = 50 # pick an horizon which is sufficient to reach the goal\n",
        "\n",
        "Q_test = backward_induction(test_env.P,test_env.R,H,gamma=1.0)\n",
        "\n",
        "state, _ = test_env.reset()\n",
        "test_env.enable_rendering()\n",
        "for h in range(H):\n",
        "  action = np.argmax(Q_test[h, state])\n",
        "  next_state, reward, term, trunc, info = test_env.step(action)\n",
        "  if term:\n",
        "    break\n",
        "  state = next_state\n",
        "\n",
        "# save video (run next cell to visualize it)\n",
        "test_env.save_gif('gridworld_backward_induction.gif')\n",
        "# clear rendering data\n",
        "test_env.clear_render_buffer()\n",
        "test_env.disable_rendering()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UQsgLAGDidJ"
      },
      "outputs": [],
      "source": [
        "Image(open('gridworld_backward_induction.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8xtdOayzhWL"
      },
      "source": [
        "# Implementation of UCBVI\n",
        "\n",
        "The UCBVI algorithm works as follows:\n",
        "\n",
        "* In each episode $t$, the agent has observed $n_t$ transitions $(s_i, a_i, r_i, s_{i+1})_{i=1}^{n_t}$ of states, actions, rewards and next states.\n",
        "* We estimate a model of the MDP as:\n",
        "$$\n",
        "\\mathbf{rewards:}\\quad\\widehat{R}_{t(s, a)} = \\frac{1}{N_t(s, a)} \\sum_{i=1}^{n_t} \\mathbb{1}\\{s = s_i, a = a_i\\} r_i\n",
        "\\\\\n",
        "\\mathbf{transitions:}\\quad \\widehat{P}_t(s\\prime|s, a) =  \\frac{1}{N_t(s, a)} \\sum_{i=1}^{n_t} \\mathbb{1}\\{s = s_i, a = a_i, s\\prime=s_{i+1}\\}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "N_t(s, a) = \\max\\left(1, \\sum_{i=1}^{n_t} \\mathbb{1}\\{s = s_i, a = a_i\\} \\right)\n",
        "$$\n",
        "* We define an appropriate exploration bonus, which is some function of the number of visits, for example\n",
        "$$\n",
        "B_t(s, a) \\propto \\sqrt{\\frac{1}{N_t(s, a)}} \\cdot\n",
        "$$\n",
        "\n",
        "* Then, in episode $t$, we compute $\\{Q_h^t(s, a)\\}_{h=1}^H$ as the ($H$-horizon) optimal value functions in the MDP whose transitions are $\\widehat{P}_t$ and whose rewards are $(\\widehat{R}_t + B_t)$. At step $h$ of episode $t$, the agent chooses the action $a_h^t \\in \\arg\\max_a Q_h^t(s, a)$.\n",
        "\n",
        "**Complete the code below to implement UCBVI.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi3Uw-mHyTbF"
      },
      "outputs": [],
      "source": [
        "# An example of bonus function\n",
        "def bonus(N):\n",
        "    \"\"\"input : a numpy array (nb of visits)\n",
        "    output : a numpy array (bonuses)\"\"\"\n",
        "    nn = np.maximum(N, 1)\n",
        "    return np.sqrt(1.0/nn)\n",
        "\n",
        "# The UCB-VI algorithm\n",
        "def UCBVI(env,H, nb_episodes,verbose=\"off\",bonus_function=bonus,gamma=1):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        env: environement\n",
        "        bonus_function : maps the number of visits to the corresponding bonus\n",
        "        H: horizon\n",
        "        gamma: discount factor. Usually set to 1.0 in finite-horizon problems\n",
        "\n",
        "    Returns:\n",
        "        episode_rewards: a vector storing the sum of rewards obtained in each episode\n",
        "        states_visited: a vector storing the number of states/action pairs visited until each episode\n",
        "        N_sa : array of size (S,A) giving the total number of visits in each state\n",
        "        Rhat : array of size (S,A) giving the estimated average rewards\n",
        "        Phat : array of size (S,A,S) giving the estimated transition probabilities\n",
        "        optimistic_Q : array of size (H,S,A) giving the optimistic Q function used in the last episode\n",
        "    \"\"\"\n",
        "    S = env.observation_space.n\n",
        "    A = env.action_space.n\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sa = np.zeros((S,A), dtype=int) # number of visits\n",
        "\n",
        "    N_sas = np.zeros((S,A,S), dtype=int) # number of transitions\n",
        "    S_sa = np.zeros((S,A)) # cumulative rewards\n",
        "    episode_rewards = np.zeros((nb_episodes,))\n",
        "    states_visited = np.zeros((nb_episodes,))\n",
        "\n",
        "\n",
        "    for k in range(nb_episodes):\n",
        "        sum_rewards = 0\n",
        "        ### TO BE COMPLETED: RUN AN EPISODE OF UCV-VI\n",
        "\n",
        "        # update sum of rewards and number of visits\n",
        "        episode_rewards[k] = sum_rewards\n",
        "        states_visited[k] = (N_sa.sum(axis=1) > 0).sum()\n",
        "\n",
        "        if (verbose==\"on\"):\n",
        "          # periodically display the rewards collected and visited states\n",
        "          if k % 50 == 0 or k==1:\n",
        "            print(\"rewards in episode {}: {}\".format(k, episode_rewards[k]), end = \", \")\n",
        "            print(\"Number of visited states: \", states_visited[k] )\n",
        "            # print(V[0, :])\n",
        "\n",
        "    return episode_rewards, states_visited, N_sa, Rhat, Phat,optimistic_Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVs9MsdBDidJ"
      },
      "source": [
        "**1. To check whether the algorithm is working, you can visualize the amount of rewards gathered in episodes with the default bonus, as well as the number of visited states since the beginning (which measures how well the environment is being explored). Based on your findings, how many episodes seem necessary for the algorithm to behave well?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZON = 200\n",
        "NUM_EPISODES = 500\n",
        "\n",
        "env = ScaleRewardWrapper(get_discrete_mountain_car_env())\n",
        "rewards, cum_visits,N_sa, Rhat, Phat, optimistic_Q = UCBVI(env, H=HORIZON, nb_episodes=NUM_EPISODES,verbose=\"on\")"
      ],
      "metadata": {
        "id": "tjkmTlnUkqBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([np.cumsum(rewards)[episode]/(episode+1) for episode in range(NUM_EPISODES)])\n",
        "#plt.plot(rewards)\n",
        "plt.title(\"average reward per episode\")\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(cum_visits)\n",
        "plt.title(\"number of states visited since the first episode\")"
      ],
      "metadata": {
        "id": "is3rP4tlnvKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. UCBVI's first purpose is not to output a candidate optimal policy, but if you want to do so, what is (are) reasonnable candidate(s)? You can check by looking how the agent behaves under this (these) policies, i.e. whether it solves the task. You can also display a 2D visualization of associated Q functions.**"
      ],
      "metadata": {
        "id": "weDQuXIwHcXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = ## choose the policy to visualize\n",
        "record_video(env,\"UCB-VI-optimistic\", HORIZON,policy)\n",
        "Video('gym_videos/UCB-VI-optimistic/rl-video-episode-0.mp4', embed=True)"
      ],
      "metadata": {
        "id": "exG498Ig1SNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the value of this policy is not possible, as it would require to perform policy evaluation, and as P and R are unknown, Monte-Carlo evaluation is the only option. Let's keep this for later. For now a proxy is to visualize $\\max_{a} Q(s,a)$ when $\\pi$ is greedy wrt to $Q$. For the optimistic policy, $Q$ is the optimistic Q function.\n"
      ],
      "metadata": {
        "id": "BlKRwqFg1-0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q_function = ## here goes Q such that policy =greedy(Q)\n",
        "value = np.max(Q-function,axis=2)\n",
        "plt.scatter(env.discretized_states[0, :], env.discretized_states[1, :], c=value[0, :], s=400)\n",
        "plt.xlabel('Car Position')\n",
        "plt.ylabel('Car Velocity')\n",
        "clb=plt.colorbar()\n",
        "clb.ax.set_title('Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yKq-fCMe3UP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In theory, any algorithm with an upper bound $\\mathbb{E}[R_T] \\leq B_T$ on its expected regret can be used to output an $\\varepsilon$-optimal policy with high probability using the following trick: run the algorithm for a sufficiently large $T$ and output a policy $\\widehat{\\pi}_T$ chosen at random among the first $T$ policies used by the algorithm. The point of this question is to quantify how long.    \n",
        "\n",
        "**3. [Theory] For a given $T$, give an upper bound on the probability $\\mathbb{P}(V^\\star - V^{\\widehat{\\pi}_T} > \\varepsilon)$ that depends on $T,\\varepsilon$ and $B_T$ (you can use Markov's inequality). Deduce the order of magnitude of the number of episodes needed by UCB-VI to output a $\\varepsilon$-optimal policy with probability $1-\\delta$. Comment on the result.**"
      ],
      "metadata": {
        "id": "ATga-bDDXDze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. UCB-VI has actually some guarantees in terms of regret, so you can try to optimize the algorithm for the setting it is designed for.**\n",
        "\n",
        "Explore different kind of bonusses for UCB-VI, in terms of resulting *expected* cumulative rewards. You may also compare bonus-based \"directed\" exploration to a model-based algorithm using instead $\\epsilon$-greedy exploration."
      ],
      "metadata": {
        "id": "1_CcyiJR4Ckg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. [Theory] Our goal was to find a policy that manages to get the car up the hill. We chose to model this as solving an episodic MDP with a large enough horizon $H$. Could this task also be modelled as solving a discounted MDP? What value would you choose for the discount?**"
      ],
      "metadata": {
        "id": "pntcNbgZKA-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Compare the best model-based algorithm found in question 4. to a model-free alternative (still in terms of cumulative rewards).**\n",
        "\n",
        "The idea is to try to make some variant of the (discounted) Q-Learning algorithm work for this problem. Learning will however still proceed in episodes of length H, with a reset, and we will similarly monitor the (undiscounted) amount of rewards gathered in each episode, for comparison with UCBVI.\n",
        "\n",
        "You may explore one of the following possibilities: combining $\\varepsilon$-greedy exploration with a careful initialization of the Q-values, or adding some exploration bonuses to the rewards.\n",
        "\n",
        "Careful describe the chosen algorithm as well as the results.\n"
      ],
      "metadata": {
        "id": "vqir4Q_K5S9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. [Theory] We would now like to compare the sample complexity of the model-free algorithm to that of the model-based algorithm. How would you proceed to do that?**\n",
        "\n",
        "\n",
        "(you are not required to actually do it, just specify how you would proceed given the time)"
      ],
      "metadata": {
        "id": "L8M_0jsobaH6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}